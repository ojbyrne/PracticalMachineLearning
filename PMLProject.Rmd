---
title: 'Practical Machine Learning Project: Human Activity Recognition'
author: "Owen Byrne"
date: "December 6, 2014"
output:
  html_document:
    self_contained: no
---
# Executive Summary #
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts (including the author) who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 

The aim of this report was to use data from accelerometers placed on the belt, forearm, arm, and dumbell of six participants to predict how well they were doing the exercise in terms of the classification in the data. 
```{r, echo=FALSE}
library(caret)
library(corrplot)
library(kernlab)
library(knitr)
library(randomForest)
```
```{r}
directory <- "data"
training_file <- "training.csv"
testing_file <- "testing.csv"

if (!file.exists(directory)) {
    dir.create(directory)
}

# curl option needed on OSX to access https urls
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", destfile = paste("./", directory, "/", training_file, sep=""), method="curl", quiet=TRUE)
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", destfile = paste("./", directory, "/", testing_file, sep=""), method="curl", quiet=TRUE)
```
Training data was read from the csv file, and some data cleaning was done. The first seven columns (data specific to each record, such as name, timestamps, etc) were removed. Columns that contained any NAs were also removed. This left a total of 59 columns + the "classe" column to be predicted.

```{r}
data_training <- read.csv(paste("./", directory, "/", training_file, sep=""), na.strings= c("NA",""," "))
data_training_clean <- data_training[8:length(data_training)]
data_training_NAs <- apply(data_training_clean, 2, function(x) {sum(is.na(x))})
data_training_clean <- data_training_clean[,which(data_training_NAs == 0)]
ncol(data_training_clean)
```
The cleaned data was then split into training and cross-validation sets.
```{r}
inTrain <- createDataPartition(y = data_training_clean$classe, p = 0.7, list = FALSE)
training <- data_training_clean[inTrain, ]
validation <- data_training_clean[-inTrain, ]
```
A random forest model was selected first to predict the classification because it has methods for balancing error in class population unbalanced data sets. One downside is fitting the model was very slow. Because the random forest model was so slow, a second model using boosting was done.

The correlation between any two trees in the forest increases the forest error rate. Therefore, a correllation plot was produced in order to see how strong the variables relationships are with each other (Exhibit 1).

```{r}
system.time(model1 <- train(classe ~ ., data=training, method="rf"))
model1
system.time(model2 <- train(classe ~ ., data=training, method="gbm", verbose=FALSE))
model2

```
## Cross-validation ##
the validation set was then predicted with the 2 models, and a confusion matrix produced, showing a very high accuracy (99.81%) for random forests, and a slightly lower accuracy for boosting.
```{r}
predictRFCrossVal1 <- predict(model1, validation)
confusionMatrix(validation$classe, predictRFCrossVal1)
predictRFCrossVal2 <- predict(model2, validation)
confusionMatrix(validation$classe, predictRFCrossVal2)
```
## Predictions ##

Finally, the testing set was loaded into R, the same cleaning was done, and the two models applied to classify the test set.

```{r}
# apply the same treatment to the final testing data
data_test <- read.csv(paste("./", directory, "/", testing_file, sep=""), na.strings= c("NA",""," "))
data_test_NAs <- apply(data_test, 2, function(x) {sum(is.na(x))})
data_test_clean <- data_test[,which(data_test_NAs == 0)]
data_test_clean <- data_test_clean[8:length(data_test_clean)]

# predict the classes of the test set
answers1 <- predict(model1, data_test_clean)
answers1
answers2 <- predict(model2, data_test_clean)
answers2


```
## Conclusion ##

Both random forest and boosting provided excellent accuracy. Boosting was a little faster and slightly less accurate. Both methods scored perfectly on the testing set. 

-----
# Exhibit 1 #

```{r, fig.height = 8, fig.width = 8}
correlMatrix <- cor(training[, -length(training)])
corrplot(correlMatrix, order = "FPC", method = "circle", type = "lower", tl.cex = 0.8,  tl.col = rgb(0, 0, 0))
```
